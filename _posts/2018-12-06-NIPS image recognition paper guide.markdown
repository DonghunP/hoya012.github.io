---
layout: post
title:  “NeurIPS 2018 image recognition paper list guide”
date:   2018-12-06
description: NeurIPS 2018 논문 중 이미지 인식과 관련있는 논문 리스트에 대해 제 주관적으로 정리하였습니다.
comments: true
---

안녕하세요, 이번 포스팅에서는 올해 12월 3일 ~ 8일 캐나다 몬트리올에서 개최된 저희에게는 NIPS로 잘 알려져있는 
 <a href="https://nips.cc/Conferences/2018/Dates" target="_blank"><b> NeurIPS 2018 </b></a> 
학회의 논문 중에 이미지 인식과 관련이 있는 논문 24편에 대해 제 주관적으로 리스트를 정리해보았습니다. 

우선 전체 accepted paper가 1011 편이다보니 하나하나 읽어보는 것은 불가능하여서, 제가 제목만 보고 재미있을 것 같은 논문 위주로 정리를 해보았습니다. 
**당부드리는 말씀은 제가 정리한 논문 리스트에 없다고 재미 없거나 추천하지 않는 논문은 절대 아니고 단지 제 주관에 의해 정리된 것임을 강조드리고 싶습니다.**

<blockquote> NeurIPS 2018 Paper Statistics </blockquote>
올해 NeurIPS 학회는 등록이 시작된지 1시간이 되지 않아서 마감이 될 만큼 굉장히 인기있는 학회였는데요, 그래서 제 주변에서도 가고 싶었는데 가지 못하는 경우를 봤습니다.
그만큼 인기가 높아진 학회인데요, 이 학회에는 매년 몇 편의 논문이 accept되는 지 조사를 해보았습니다.

<figure>
	<img src="{{ '/assets/img/nips_2018/NIPS_acceptance.PNG' | prepend: site.baseurl }}" alt=""> 
	<figcaption> [최근 5년간 NIPS acceptance rate 비교] </figcaption>
</figure> 

매년 제출되는 논문 편수도 증가하고 있고, 그에 따라서 accept되는 논문들의 편수도 증가를 하고 있습니다. 올해의 전체 제출된 논문은 5년전의 약 3배, 2년전의 약 2배로 굉장히 규모가 커진 것을 확인할 수 있습니다.
또한 약 20% 초반의 acceptance rate를 보이고 있는 것을 확인할 수 있습니다.

참고로 올해는 총 30편의 oral paper와 168편의 spotlight paper, 그리고 813편의 poster 총 1011편의 논문이 accept되었으며, 저는 오늘 그 중 24편의 논문을 소개드리고자 합니다.

<blockquote> Image Recognition 관련 논문 소개 </blockquote>  

앞서 말씀드렸듯이 1011편을 다 확인하기엔 시간과 체력이 부족하여서, 간단하게 제목만 보면서 제가 느끼기에 재미가 있을 것 같은 논문들을 추려보았습니다.
총 24편의 논문이며, 1편의 oral paper, 6편의 spotlight paper, 17편의 poster paper로 준비를 해보았습니다. 또한 각 논문마다 abstract를 읽고 논문을 간단히 정리해보았습니다.

##  <a href="http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf" target="_blank"><b> 1.	How Does Batch Normalization Help Optimization? (Oral)  </b></a>  

##  <a href="http://papers.nips.cc/paper/8160-sanity-checks-for-saliency-maps.pdf" target="_blank"><b> 2.	Sanity Checks for Saliency Maps (Spotlight)  </b></a>  

##  <a href="http://papers.nips.cc/paper/7928-a-probabilistic-u-net-for-segmentation-of-ambiguous-images.pdf" target="_blank"><b> 3.	A Probabilistic U-Net for Segmentation of Ambiguous Images (Spotlight)  </b></a>  

##  <a href="http://papers.nips.cc/paper/8277-bias-and-generalization-in-deep-generative-models-an-empirical-study.pdf" target="_blank"><b> 4.	Bias and Generalization in Deep Generative Models: An Empirical Study (Spotlight) </b></a>  

##  <a href="http://papers.nips.cc/paper/7485-norm-matters-efficient-and-accurate-normalization-schemes-in-deep-networks.pdf" target="_blank"><b> 5.	Norm matters: efficient and accurate normalization schemes in deep network (Spotlight)  </b></a>  

##  <a href="http://papers.nips.cc/paper/7935-gilbo-one-metric-to-measure-them-all.pdf" target="_blank"><b> 6.	GILBO: One Metric to Measure Them All (Spotlight)  </b></a>  

##  <a href="http://papers.nips.cc/paper/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks.pdf" target="_blank"><b> 7.	A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks (Spotlight)  </b></a>  

##  <a href="http://papers.nips.cc/paper/7766-channelnets-compact-and-efficient-convolutional-neural-networks-via-channel-wise-convolutions.pdf" target="_blank"><b> 8.	ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions </b></a>  

##  <a href="http://papers.nips.cc/paper/7454-collaborative-learning-for-deep-neural-networks.pdf" target="_blank"><b> 9.	Collaborative Learning for Deep Neural Networks  </b></a>  

##  <a href="http://papers.nips.cc/paper/7879-l4-practical-loss-based-stepsize-adaptation-for-deep-learning.pdf" target="_blank"><b> 10.	L4: Practical loss-based stepsize adaptation for deep learning  </b></a>  

##  <a href="http://papers.nips.cc/paper/7761-scalable-methods-for-8-bit-training-of-neural-networks.pdf" target="_blank"><b> 11.	Scalable Methods for 8-bit Training of Neural Networks  </b></a>  

##  <a href="http://papers.nips.cc/paper/7647-adversarial-examples-that-fool-both-computer-vision-and-time-limited-humans.pdf" target="_blank"><b> 12.	Adversarial Examples that Fool both Computer Vision and Time-Limited Humans  </b></a>  

##  <a href="http://papers.nips.cc/paper/8072-co-teaching-robust-training-of-deep-neural-networks-with-extremely-noisy-labels.pdf" target="_blank"><b> 13.	Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels   </b></a>  

##  <a href="http://papers.nips.cc/paper/8183-deep-anomaly-detection-using-geometric-transformations.pdf" target="_blank"><b> 14.	Deep Anomaly Detection Using Geometric Transformations  </b></a>  

##  <a href="http://papers.nips.cc/paper/7828-practical-deep-stereo-pds-toward-applications-friendly-deep-stereo-matching.pdf" target="_blank"><b> 15.	Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching.  </b></a>  

##  <a href="http://papers.nips.cc/paper/7921-bayesian-adversarial-learning.pdf" target="_blank"><b> 16.	Bayesian Adversarial Learning  </b></a>  

##  <a href="http://papers.nips.cc/paper/7429-bilinear-attention-networks.pdf" target="_blank"><b> 17.	Bilinear Attention Networks  </b></a>  

##  <a href="http://papers.nips.cc/paper/7915-generative-probabilistic-novelty-detection-with-adversarial-autoencoders.pdf" target="_blank"><b> 18.	Generative Probabilistic Novelty Detection with Adversarial Autoencoders   </b></a>  

##  <a href="http://papers.nips.cc/paper/8143-sniper-efficient-multi-scale-training.pdf" target="_blank"><b> 19.	SNIPER: Efficient Multi-Scale Training  </b></a>  

##  <a href="http://papers.nips.cc/paper/7798-to-trust-or-not-to-trust-a-classifier.pdf" target="_blank"><b> 20.	To Trust Or Not To Trust A Classifier  </b></a>  

##  <a href="http://papers.nips.cc/paper/7448-enhancing-the-accuracy-and-fairness-of-human-decision-making.pdf" target="_blank"><b> 21.	Enhancing the Accuracy and Fairness of Human Decision Making  </b></a>  

##  <a href="http://papers.nips.cc/paper/7394-adversarial-vulnerability-for-any-classifier.pdf" target="_blank"><b> 22.	Adversarial vulnerability for any classifier  </b></a>  

##  <a href="http://papers.nips.cc/paper/7338-how-to-start-training-the-effect-of-initialization-and-architecture.pdf" target="_blank"><b> 23.	How to Start Training: The Effect of Initialization and Architecture  </b></a>  

##  <a href="http://papers.nips.cc/paper/8007-neural-architecture-optimization.pdf" target="_blank"><b> 24.	Neural Architecture Optimization  </b></a>  


<blockquote> 참고 문헌 </blockquote>  
- <a href="https://github.com/lixin4ever/Conference-Acceptance-Rate" target="_blank"> Statistics of acceptance rate for the main AI conferences </a>
